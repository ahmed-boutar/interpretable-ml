{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmed-boutar/interpretable-ml/blob/main/interpretable_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Description\n",
        "\n",
        "In this notebook, we will use a dataset from a telecommunications company (https://www.kaggle.com/datasets/blastchar/telco-customer-churn/code). The company is interested in understanding the factors that contribute to customer churn (customers leaving the company for a competitor) and developing interpretable models to predict which customers are at risk of churning.\n",
        "\n",
        "The license of the dataset belongs to the Original Authors (Data files Â© Original Authors)\n",
        "\n",
        "In terms of models, we will focus on creating a linear regression model, a logisitic regression model, and a GAM model. Our target variable is 'Churn', which is a binary variable, effectively making this problem a classification problem. Using the linear regression model is not optimal in this case, but might give us some interesting insights since learning about the interpretability of inherently interpretable models is interesting ðŸ˜ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please use this to connect your GitHub repository to your Google Colab notebook\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"interpretable-ml\" # Change to your repo name\n",
        "git_path = 'https://github.com/ahmed-boutar/interpretable-ml.git' #Change to your path\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#%!pip install -r \"requirements.txt'\" #Add if using requirements.txt\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "# notebook_dir = 'src'\n",
        "# path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{repo_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Since pygam is the only dependency not installed in Colab\n",
        "#Installing just pygam instead of everything in the requirements.txt helps us avoid restarting \n",
        "#the colab session\n",
        "!pip install pygam --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Our goal in this step is to:\n",
        "- Get an overview of the dataset\n",
        "- Understand the relationships between the different features and the target variable (churn)\n",
        "- Determine was assumptions about linear, logistic, and GAM models are met (for the assumptions that can be effectively checked before modeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling missing and duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.duplicated().sum()\n",
        "df.drop_duplicates(inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see we have no missing/null values in our dataset. Our dataset does not have any duplicate values neither"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descriptive Stats "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of our features such as Total Charges have a dtype object, while only representing numbers. Those features will be converted to the appropriate dype (float in this case), thereby excluding them from the list of categorical features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Convert the column TotalCharges from object to a float dtype \n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "# Explicitely setting the type to float\n",
        "df['TotalCharges'] = df['TotalCharges'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the count of our total charges has decreased, meaning some values were set to NaN. Let's inspect it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df['TotalCharges'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The monthly charges were set to NaN for customers that have been enrolled with the company for 0 months (tenure equals 0). Since the charges have not added up to total charges, we can just fill out the total charges we expect, i.e their current monthly charges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill NaN values in TotalCharges with values from MonthlyCharges\n",
        "df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n",
        "df[df['TotalCharges'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "description = df.describe()\n",
        "description.loc['mode'] = df.apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "print(description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our numeric data is skewed for our numerical features. For example, the total charges exhibit right skewness since the mode < median < mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distributions of Numerical and Categorical Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of Numerical Features\n",
        "\n",
        "Since SeniorCitizen is a binary feature (although encoded as a numeric once), we will include plotting its distribution among the categorical features. \n",
        "\n",
        "We will plot the distrubtion of tenure, MonthlyCharges, and TotalCharges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "binary_variables = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, var in enumerate(binary_variables):\n",
        "    plt.subplot(2, 3, i + 1)  # Create subplots, adjust the grid if needed\n",
        "    sns.histplot(df[var], kde=True)  # Add a Kernel Density Estimate (KDE) for better visualization\n",
        "    plt.title(f'Histogram of {var}')  # Set title for each subplot\n",
        "plt.tight_layout()  # Adjust subplots to fit into figure area.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the distributions above, we can confirm our assumption that the TotalCharges variable is rightly skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will plot some of the categorical features agains the target variable 'Churn' using bar plots. We will also use contingency tables for some of the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [],
      "source": [
        "boxplot_vars = ['TechSupport', 'Contract', 'PaymentMethod']\n",
        "\n",
        "plt.figure(figsize=(24, 10))\n",
        "for i, var in enumerate(boxplot_vars):\n",
        "    plt.subplot(2, 3, i + 1) \n",
        "    sns.countplot(x=var, hue='Churn', data=df)  \n",
        "    plt.title(f'{var} Count by Churn') \n",
        "plt.tight_layout() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Separate categorical features\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.difference(['customerID', 'Churn'])\n",
        "\n",
        "# Dictionary that will be used later to drop the columns with the lowest p-values when performing \n",
        "# feature selection\n",
        "feature_p_val_dict = {}\n",
        "# Create the contingency table\n",
        "# Asked Claude Heroku for help to create the contingency table \n",
        "for feature in categorical_features:\n",
        "    contingency_table = pd.crosstab(df[feature], df['Churn'])\n",
        "\n",
        "    # Add row and column totals\n",
        "    contingency_table['Total'] = contingency_table.sum(axis=1)\n",
        "    contingency_table.loc['Total'] = contingency_table.sum()\n",
        "\n",
        "    # Perform chi-square test\n",
        "    chi2, p_value, _, _ = chi2_contingency(contingency_table.iloc[:-1, :-1])\n",
        "    feature_p_val_dict[feature] = p_value\n",
        "    print(f\"Contingency Table Count of {feature} vs Churn:\")\n",
        "    print(contingency_table)\n",
        "    print(f\"\\nChi-square statistic: {chi2:.4f}\")\n",
        "    print(f\"p-value: {p_value:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Adding the SeniorCitizen column to the categorical features for the sake of creating contingency tables\n",
        "contingency_table = pd.crosstab(df['SeniorCitizen'], df['Churn'])\n",
        "\n",
        "# Add row and column totals\n",
        "contingency_table['Total'] = contingency_table.sum(axis=1)\n",
        "contingency_table.loc['Total'] = contingency_table.sum()\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p_value, _, _ = chi2_contingency(contingency_table.iloc[:-1, :-1])\n",
        "feature_p_val_dict['SeniorCitizen'] = p_value\n",
        "print(f\"Contingency Table Count of SeniorCitizen vs Churn:\")\n",
        "print(contingency_table)\n",
        "print(f\"\\nChi-square statistic: {chi2:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As indicated by the chi-squared test, we can see that some of our feature variables have a relationship with the a target variable (where p<0.05 and chi-square statistic is high). It is interesting to see that PhoneService and Gender had a high p-value, indicating the absence of a relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check for multicollinearity, we will use a correlation matrix (heatmap). Given that the matrix relies on numerical values, we will first need to encode our categorical variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify columns to exclude\n",
        "exclude_columns = ['customerID']\n",
        "\n",
        "# Separate categorical features that will be encoded later on\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.difference(exclude_columns)\n",
        "# Get possible values for each categorical feature and visualize it in the console\n",
        "feature_values = {feature: df[feature].unique() for feature in categorical_features}\n",
        "for feature in feature_values.keys():\n",
        "    print(feature + \" -> \" + str(feature_values[feature]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For binary variables, we will perform an encoding in place where No is mapped to 0 and Yes is mapped to 1. \n",
        "\n",
        "For categorical variables with more than 2 values, we will perform one hot encoding. We will manually drop some of the columns it generates based on that value's frequency we found using the contingency tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Separate categorical features into binary and non-binary features\n",
        "binary_features = [col for col in categorical_features if len(df[col].unique()) == 2]\n",
        "non_binary_features = [col for col in categorical_features if len(df[col].unique()) > 2]\n",
        "\n",
        "#Create the mapping for each column\n",
        "binary_map = {'Yes': 1, 'No': 0, 'Male': 0, 'Female': 1}\n",
        "\n",
        "# Encode the binary features with the appropriate values given the encoding map\n",
        "for feature in binary_features:\n",
        "    df[feature] = df[feature].map(binary_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that when doing the one hot encoding, we are setting drop_first is set to False in order to preserve all the newly created columns. We will manually drop some of the columns using a function of predefined values to drop. \n",
        "\n",
        "Remember the goal here is to create accurate enough models that are interpretable, meaning we need to find a balance between complexity and interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode_columns(df):\n",
        "    # Perform one-hot encoding\n",
        "    df_encoded = pd.get_dummies(df, columns=non_binary_features, drop_first=False)\n",
        "    drop_categories = {\n",
        "        'InternetService': ['No'],\n",
        "        'OnlineBackup': ['No internet service'],\n",
        "        'OnlineSecurity': ['No internet service'],\n",
        "        'StreamingMovies': ['No internet service'],\n",
        "        'StreamingTV': ['No internet service'],\n",
        "        'TechSupport': ['No internet service'],\n",
        "        'DeviceProtection': ['No internet service'],\n",
        "        'MultipleLines': ['No phone service'],\n",
        "        'Contract': ['One year']\n",
        "    }\n",
        "    \n",
        "    # Drop unwanted columns\n",
        "    columns_to_drop = []\n",
        "    for feature, categories in drop_categories.items():\n",
        "        for category in categories:\n",
        "            column_to_drop = f\"{feature}_{category}\"\n",
        "            if column_to_drop in df_encoded.columns:\n",
        "                columns_to_drop.append(column_to_drop)\n",
        "    \n",
        "    df_encoded.drop(columns=columns_to_drop, inplace=True)\n",
        "    return df_encoded\n",
        "\n",
        "df_encoded = one_hot_encode_columns(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation heatmap with binary and numeric feature values, which include the continuous variables as well as the binary features that were encoded as numeric. Note that this does not include the one hot encoded feature variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_correlation_heatmap(df):\n",
        "    # Select only numeric columns for correlation analysis\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Compute the correlation matrix\n",
        "    corr_matrix = numeric_df.corr()\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(24, 12))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "#using the df not df_encoded to exclude the one_hot_encoded columns, but include the binary features\n",
        "# that were encoded as numeric where 0 means No\n",
        "print_correlation_heatmap(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that changes all the boolean columns to numeric ones\n",
        "def parse_bool_features_as_numeric(df):\n",
        "    boolean_cols = df.select_dtypes(include=['bool']).columns\n",
        "    df[boolean_cols] = df[boolean_cols].astype(int)\n",
        "    return df\n",
        "\n",
        "df_encoded = parse_bool_features_as_numeric(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that prints the heatmap for all columns by using the numeric columns and changing the dtype of the boolean features to numeric to include them in the heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_correlation_heatmap(df_encoded)\n",
        "print(df_encoded.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see through the corellation matrix, our features show signs of multicollinearity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking if Assumptions Were Met "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have explored our dataset and uncovered some of the relationships between our feature variables and target, we can now check what assumptions were met. We will revisit some of these assumptions later on once we proceed with feature engineering/selection and actually create the models to plot the residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Linearity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to check if a linear relationship between features and the target variable exist, we will plot the relationship between some of the feature variables and the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Select all numerical features while excluding the target. The goal is to look for linearity \n",
        "#the features and the target\n",
        "numerical_features = df.select_dtypes(include=[np.number]).columns.difference(['Churn'])\n",
        "print(numerical_features)\n",
        "print(len(numerical_features))\n",
        "print(len(df.columns))\n",
        "\n",
        "for feature in ['MonthlyCharges', 'TotalCharges', 'tenure']:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(df[feature], df['Churn'], c=df['Churn'].map({0: 'blue', 1: 'red'}), alpha=0.5)\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Target')\n",
        "    plt.title('Scatter Plot of Feature vs. Target')\n",
        "\n",
        "        \n",
        "    plt.show()\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As stated here (https://bookdown.org/rwnahhas/RMPH/mlr-linearity.html), the linearity assumption is met by default since our target variable is binary. Therefore, we have confirmed that the linearity assumption is met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Independence of Observations\n",
        "\n",
        "After looking at the observations, representing a distinct customer each, it is reasonable to assume that they are independent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### No Multicollinearity\n",
        "\n",
        "As stated in our correlation analysis, our feature variables show multicollinearity. We will revisit this aassumption once we proceed with feature engineering/selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### No Endogeneity\n",
        "\n",
        "By looking at the dataset, one can see that all potential relevant variables are included in the dataset. Since we are not dropping any columns with a low p-value, we can safely assume that endogeneity is met\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### No Autocorrelation\n",
        "\n",
        "Since our dataset is static autocorrelation is not a major concern (unlike with time-series datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Large Sample Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the number of observations we have, it is safe to assume that this assumption was met"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Absence of Perfect Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the information gathered through contingency tables and the bar plots for categorical features, we can see that there is no perfect separation, i.e none of our our predictors perfectly predicts our target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Homoscedasticity (constant variance)\n",
        "\n",
        "In order to check the Homoscedasticity, i.e that residuals are uncorrelated random variables with 0 mean and constant variance, we will first create a regression model and check for our assumption later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normality\n",
        "\n",
        "The residuals of the model should be normally distributed. We will check if the assumption was met once we create our models and plot the residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have explored our dataset and uncovered some relationships between our feature variables and our target variables, we can start our feature selection/feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering/Selection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can start by dropping the features with a lot p-value since they don't have a relationship with the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {},
      "outputs": [],
      "source": [
        "# columns_to_drop = []\n",
        "# for feature in feature_p_val_dict:\n",
        "#     if feature_p_val_dict[feature] > 0.05: #drop features that have p-value > 0.05\n",
        "#         columns_to_drop.append(feature)\n",
        "\n",
        "# print(columns_to_drop)\n",
        "# df.drop(columns=columns_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop the total charges since we can find it from the data (tenure * monthly charges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded.drop(columns=['TotalCharges'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_correlation_heatmap(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the monthly charges are correlated to the services the customer is signed up for, we can drop the monthly charges as we can infer it from the number of services the customer is signed up for. \n",
        "\n",
        "Since streaming movies and TV are also correlated, we should drop one of the columns, in this case streaming movies. Most users that are streaming TV, are streaming movies. \n",
        "\n",
        "Lastly, we will drop the column DSL representing the value DSL for the Internet Service. Keeping the optical fiber feature makes more since it affects the monthly charges, and would still allow us to see whether the customer has internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded.drop(columns=['MonthlyCharges', 'StreamingMovies_Yes'], inplace=True)\n",
        "# df_encoded.drop(columns=['StreamingMovies_Yes'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_correlation_heatmap(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the contracts to reflect the number of months as to decrease the colinearity with the tenure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded['AutomaticPayment'] = df_encoded.apply(lambda row: 1 if row['PaymentMethod_Bank transfer (automatic)'] else 1 if row['PaymentMethod_Credit card (automatic)'] else 0, axis=1)\n",
        "df_encoded.drop(columns=['PaymentMethod_Bank transfer (automatic)', 'PaymentMethod_Credit card (automatic)', 'PaymentMethod_Mailed check', 'PaymentMethod_Electronic check'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_correlation_heatmap(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error,r2_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = 'Churn'\n",
        "X = df_encoded.drop(columns=[target, 'customerID'])\n",
        "y = df_encoded[target]\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function will be used to evaluate the linear regression model using metrics like MSE and R^2. \n",
        "\n",
        "The code for the evalutation function, residuals plotting, and using the Ridge Regression was inspired from this notebook provided by Dr.Brinnae Bent (Source: https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/interpretable-ml-example-notebooks/regression-interpretability.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_linear_regression_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"R2 Score: {r2:.2f}\")\n",
        "    \n",
        "    coefficients = model.coef_\n",
        "    coef_df = pd.DataFrame({'Predictor': X.columns, 'Coefficient': coefficients})\n",
        "    coef_df = coef_df.sort_values('Coefficient', ascending=True)\n",
        "  \n",
        "    return model, y_pred, coef_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the function, which trains the model and evaluates it. It also prints the different coefficients the model has assigned to each feature variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_model, lr_y_pred, coef_df = evaluate_linear_regression_model(LinearRegression(), X_train, X_test, y_train, y_test, \"Linear Regression\")\n",
        "print(coef_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge_model, ridge_pred, ridge_coef_df = evaluate_linear_regression_model(Ridge(alpha=1.0), X_train, X_test, y_train, y_test, \"Ridge Regression\")\n",
        "print(ridge_coef_df)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coefficient Comparison Plot\n",
        "def plot_coefficients(lr_model, ridge_model):\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Linear': lr_model.coef_,\n",
        "        'Ridge': ridge_model.coef_,\n",
        "    }, index=X.columns)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    coef_df.plot(kind='bar', width=0.8)\n",
        "    plt.title('Coefficient Comparison: Linear vs Ridge vs Lasso')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Coefficient Value')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_coefficients(lr_model, ridge_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use a QQ plot and check for the Normality assumption "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "# Get the residuals (observed - predicted)\n",
        "residuals = y_test - lr_y_pred\n",
        "\n",
        "# QQ Plot\n",
        "sm.qqplot(residuals, line='45')\n",
        "plt.title(\"QQ Plot of Residuals\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see here, the residualts deviate significantly from the line (especially at the tails). This suggests that the residuals may not be normally distributed, indicating a violation of the normality assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now use the Breuschâ€“Pagan Test to check for Homoscedasticity. I learned about the existence and significance of this test by asking Claude Heroku. The code for this test was written with the help of Claude Heroku.\n",
        "\n",
        "The Breuschâ€“Pagan test checks if the residuals of a model have constant variance (homoscedasticity). In other words, it tests whether the residualsâ€™ variance depends on the values of the predictor variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "# Breusch-Pagan test\n",
        "# Add constant to features matrix (X)\n",
        "X_test_with_const = sm.add_constant(X_test)\n",
        "\n",
        "# Perform the Breusch-Pagan test\n",
        "bp_test = het_breuschpagan(residuals, X_test_with_const)\n",
        "\n",
        "# The test returns four values:\n",
        "# Lagrange Multiplier statistic, p-value, f-value, and f p-value\n",
        "lm_stat, lm_pvalue, f_stat, f_pvalue = bp_test\n",
        "\n",
        "print(f\"Lagrange Multiplier p-value: {lm_pvalue}\")\n",
        "print(f\"F-statistic p-value: {f_pvalue}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the p-values are extremely small (<0.05), we can assume that heteroscedasticity is present (i.e., the variance of residuals is not constant), thereby violating our assumption of homoscedasticity. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residual Plot\n",
        "def plot_residuals(y_test, y_pred, model_name):\n",
        "    residuals = y_test - y_pred\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_pred, residuals)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title(f'Residual Plot - {model_name}')\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_residuals(y_test, lr_y_pred, 'Linear Regression')\n",
        "plot_residuals(y_test, ridge_pred, 'Ridge Regression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpeting the results and coefficients for Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our linear regression model did not do that well, which is understandable given that our target variable is binary. \n",
        "\n",
        "In fact, the r^2 value is 0.30, meaning our model was only able to capture 30% of the variance of the target variable, which is not great for generalization.\n",
        "\n",
        "Our MSE is 0.18 is fairly low in absolute terms. However, given that we are treating our target value as a continuous variable (a range of 0 to 1), it is safe to say that the value of 0.18 for the MSE indicates a fairly high error. \n",
        "\n",
        "In order to analyze our coefficients, let's look at the positive and negative coefficients: \n",
        "- Positive Coefficients: We know that features with positive coefficients increase the likelihood of Churn as their values increase\n",
        "    - Contract_Month-to-Month with a coefficient of 0.113 suggests that customers with a month-to-month contract are more likely to churn than those on longer contracts (one-year and two-year contracts)\n",
        "    - InternetService_Fiber optic with a coefficient of 0.101192 suggests that customers with a Fiber optic service are more likely to churn than those with no internet service or those with DSL. This is a bit unexpected given that using optical fiber is much better in terms of internet quality. This may raise questions around the performance of the optical fiber service and/or its pricing by the company. \n",
        "    - PaperlessBilling with a coefficient 0.049514 (lowest positive coefficient) indicates that customers that opted to PaperlessBilling are more likely to churn, which is unexpected to some extent, given that it facilitates reviewing the bills and that most of users are not senior citizens, in which case they might like to receive their bills (\"old school\")\n",
        "    - It is important to note that our model assigned OnlineBackup_Yes a positive coefficient, meaning those that use online backups are more likely to churn. This raises questions about our feature selection or how good the online backup service the company offers is (or maybe its pricing as well). The model also assigned OnlineBackup_No a positive coefficient, which definitely raises questions about how well our model is performing (even though the coefficient's value is really close to 0)\n",
        "\n",
        "- Negative Coefficients\n",
        "    - Our model assigned TechSupport_Yes a negative coefficient, which is understandable. We expect customers that receive tech support would have an overall better experience and would likely stay with the company (i.e won't churn)\n",
        "    - StreamingMovies_No with a coefficient of -0.047 indicates that customers who donâ€™t use the streaming movies service are less likely to churn.\n",
        "    - InternetService_DSL with a coefficient of -0.043287 is aligned with the fact that the model is assigned a positive coefficient to InternetService_Fiber Optic.\n",
        "\n",
        "- Some features like gender and tenure have really small coefficients (close to zero) indicating little to no impact on customers churning in our model\n",
        "- Overall, most of our coefficients are relatively small, indicated a weak linear relationship between them and the target value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actual vs Predicted Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actual vs Predicted Plot\n",
        "def plot_actual_vs_predicted(y_test, y_pred, model_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Actual vs Predicted - {model_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_actual_vs_predicted(y_test, lr_y_pred, 'Linear Regression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function will be used to evaluate the linear regression model using metrics like Accuracy Score, Precision, Recall\n",
        "\n",
        "The code for this function was inspired from this notebook provided by Dr.Brinnae Bent (Source: https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/interpretable-ml-example-notebooks/regression-interpretability.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_logistic_regression_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)  \n",
        "    acc_score = accuracy_score(y_test,y_pred) \n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall Score: {recall:.3f}\")\n",
        "    print(f\"Accuracy Score: {acc_score:.3f}\")\n",
        "    \n",
        "    coefficients = model.coef_[0]\n",
        "    coef_df = pd.DataFrame({'Predictor': X.columns, 'Coefficient': coefficients})\n",
        "    coef_df = coef_df.sort_values('Coefficient', ascending=True)\n",
        "  \n",
        "    return model, y_pred, coef_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_reg_model, logistic_y_pred, logisitic_coef_df = evaluate_logistic_regression_model(model, X_train_scaled, X_test_scaled, y_train, y_test, 'Logistic Regression')\n",
        "print(logisitic_coef_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_actual_vs_predicted(y_test, logistic_y_pred, 'Logistic Regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_residuals(y_test, logistic_y_pred, 'Logistic Regression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpeting the results and coefficients for Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our logistic regression model shows a good enough performance (that can definitely be improved) based on the following metrics:\n",
        "\n",
        "- A precision of 0.69 means that when the model predicts a customer will churn, it is correct about 69% of the time, which may not be good enough if the company's goal requires being highly confident in churn predictions\n",
        "- A recall of 0.590 indicates that the model captures 59% of all actual churn cases, which relatively low, especially if the company's goal requires catching more true churn cases, suggesting some room for improvement in identifying customers who will churn\n",
        "- An accuracy of 0.821 means that the model correclty classifies about 82.1% of the churning cases, which is a decent overall performance. To be specific, 82.1% indicates the overall proportion of correct predictions. However, it is important to note that accuracy can be misleading if the classes are imbalanced, which is the case here since we have many more customers that do not churn than those that churn. \n",
        "\n",
        "To analyze our coefficients, let's look at the positive and negative coefficients. \n",
        "- Positive Coefficients\n",
        "\t- Contract_Month-to-Month (0.334): Customers with a month-to-month contract are significantly more likely to churn compared to those on longer contracts. This is intuitive since month-to-month customers have fewer contractual obligations.\n",
        "\t- InternetService_Fiber optic (0.334): Customers with fiber optic service are more likely to churn compared to those with no internet or DSL. This could raise questions about the quality of fiber optic service or its pricing.\n",
        "\t- PaperlessBilling (0.170): Surprisingly, customers using paperless billing are more likely to churn, which could be counterintuitive, as paperless billing is often seen as convenient.\n",
        "\t- StreamingTV_Yes (0.172): This indicates that customers who use the streaming TV service are more likely to churn, possibly because of dissatisfaction with the service.\n",
        "\n",
        "- Negative Coefficients:\n",
        "\t- tenure (-0.791): This is the most impactful feature in reducing churn. As tenure increases, customers are much less likely to churn, which makes sense since long-standing customers tend to be more loyal\n",
        "\t- Contract_Two year (-0.331): Customers with two-year contracts are less likely to churn, likely because of longer-term commitment, which aligns with the fact that the model assigned a positive coefficient to the month-to-month contract type\n",
        "\t- StreamingMovies_No (-0.173): Customers who do not use streaming movies are less likely to churn, which might indicate that customers not enrolled in extra services are more satisfied with the core services\n",
        "\t- PhoneService (-0.100): Customers with phone service are less likely to churn\n",
        "\n",
        "Features with coefficients close to zero (like gender, Partner, and TechSupport_Yes) have very little impact on churn in this model. This suggests these variables are not particularly useful in predicting churn. We should explore if these features would provide better values when combined with other predictors (through interaction terms or derived features)\n",
        "\n",
        "While the model performs reasonably well in terms of accuracy, precision, and recall, the presence of some unexpected signs (e.g., OnlineBackup_No and OnlineBackup_Yes both being positive) and some class imbalance suggests that further feature engineering or even regularization could improve the interpretability and performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generalized Additive Model (GAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pygam import LogisticGAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_gam_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)  \n",
        "    acc_score = accuracy_score(y_test,y_pred) \n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall Score: {recall:.3f}\")\n",
        "    print(f\"Accuracy Score: {acc_score:.3f}\")\n",
        "    \n",
        "  \n",
        "    return model, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to create and train the logistic GAM was inspired from the code provided by Dr.Brinnae Bent (Source: https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/interpretable-ml-example-notebooks/generalized-models-interpretability.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initilize GAM\n",
        "gam = LogisticGAM()\n",
        "\n",
        "# Find best smoothing parameters for each spline term\n",
        "gam.gridsearch(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam, gam_y_pred  = evaluate_gam_model(gam, X_train_scaled, X_test_scaled, y_train, y_test, 'Logistic GAM')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now use the Breuschâ€“Pagan Test to check for Homoscedasticity. I learned about the existence and significance of this test by asking Claude Heroku. The code for this test was written with the help of Claude Heroku.\n",
        "\n",
        "The Breuschâ€“Pagan test checks if the residuals of a model have constant variance (homoscedasticity). In other words, it tests whether the residualsâ€™ variance depends on the values of the predictor variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "residuals = y_test - gam_y_pred\n",
        "# Breusch-Pagan test\n",
        "# Add constant to features matrix (X)\n",
        "X_test_with_const = sm.add_constant(X_test)\n",
        "\n",
        "# Perform the Breusch-Pagan test\n",
        "bp_test = het_breuschpagan(residuals, X_test_with_const)\n",
        "\n",
        "# The test returns four values:\n",
        "# Lagrange Multiplier statistic, p-value, f-value, and f p-value\n",
        "lm_stat, lm_pvalue, f_stat, f_pvalue = bp_test\n",
        "\n",
        "print(f\"Lagrange Multiplier p-value: {lm_pvalue}\")\n",
        "print(f\"F-statistic p-value: {f_pvalue}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the p-values are extremely small (<0.05), we can assume that heteroscedasticity is present (i.e., the variance of residuals is not constant), thereby violating our assumption of homoscedasticity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpreting the Logistic GAM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our logistic GAM model shows a good enough performance (that can definitely be improved) based on the following metrics:\n",
        "Precision: 0.679\n",
        "Recall Score: 0.568\n",
        "Accuracy Score: 0.815\n",
        "- A precision of 0.679 means that when the model predicts a customer will churn, it is correct about 67.9% of the time, which may not be good enough if the company's goal requires being highly confident in churn predictions\n",
        "- A recall of 0.568 indicates that the model captures 56.8% of all actual churn cases, which relatively low, especially if the company's goal requires catching more true churn cases, suggesting some room for improvement in identifying customers who will churn\n",
        "- An accuracy of 0.815 means that the model correclty classifies about 81.5% of the churning cases, which is a decent overall performance. To be specific, 81.5% indicates the overall proportion of correct predictions. However, it is important to note that accuracy can be misleading if the classes are imbalanced, which is the case here since we have many more customers that do not churn than those that churn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GAM model is less interpretable compared to the linear and logistic regression models since it does not represent each feature with a single coefficient, rather by a set of basis functions that allow the model to capture non-linear relationships. To interpret this model, let's look into the partial dependence plots "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize GAM\n",
        "plt.figure(figsize=(20, 15))\n",
        "for i, term in enumerate(gam.terms):\n",
        "    if term.isintercept:\n",
        "        continue\n",
        "    plt.subplot(9, 3, i+1)\n",
        "    XX = gam.generate_X_grid(term=i)\n",
        "    plt.plot(XX[:, term.feature], gam.partial_dependence(term=i, X=XX))\n",
        "    plt.title(X.columns[i])\n",
        "    plt.ylabel('Partial dependence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We can see that the tenure has a non-linear relationship with the predicted value. With a sharp increase intitially, then leveling off, indicating that past a certain amount of time, tenure doesn't affect churning as much. \n",
        "- We can also see how having a month-to-month contract has a positive effect on churning, confirming our findings from the previous models (similar to having a yearly contract, which in this case has a negative effect) \n",
        "- InternetService_Fiber optic has a strong positive effect as the value increases, which aligns with our findings from the previous models and the p-value associated with it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gam summary provides us with the diffent p-values, giving us insights to the significance of each feature's contribution (feature function, which the sum of all the basis functions associated with that feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Highly Significant Features (p < 0.001):\n",
        "\t- tenure (s(4)): p-value = 0.00e+00\n",
        "\t- PhoneService (s(5)): p-value = 1.20e-03\n",
        "\t- PaperlessBilling (s(6)): p-value = 5.21e-05\n",
        "\t- Contract_Month-to-month (s(7)): p-value = 1.52e-08\n",
        "\t- Contract_Two year (s(8)): p-value = 1.59e-07\n",
        "\t- InternetService_Fiber optic (s(12)): p-value = 0.00e+00\n",
        "\t- OnlineSecurity_No (s(17)): p-value = 0.00e+00\n",
        "These features have a high statistical significance, indicating they have a meaningful effect on churning. Some of these features' importance in predicting the target variable are consistent across all models such as tenure, InternetService-Fiber Optic and contract_month-to-month. It is interesting to note how Online_security_no is extremely significant in this model, which is different from our other models. \n",
        "\n",
        "- Moderately Significant Features (p < 0.05):\n",
        "\t- OnlineBackup_Yes (s(16)): p-value = 2.46e-03\n",
        "\t- MultipleLines_No (s(13)): p-value = 6.32e-05\n",
        "\t- StreamingMovies_No (s(19)): p-value = 5.56e-05\n",
        "\t- TechSupport_Yes (s(23)): p-value = 1.62e-01\n",
        "These features are statistically significant but to a lesser extent compared to those with lower p-values. Their impact is less dominant in predicting churning. Some of these features such as OnlineBackup_Yes are consistently significant across all of our models.\n",
        "- Not Significant (p > 0.05):\n",
        "\t- gender (s(0)): p-value = 7.71e-01\n",
        "\t- SeniorCitizen (s(1)): p-value = 1.03e-01\n",
        "\t- Partner (s(2)): p-value = 4.26e-01\n",
        "\t- Dependents (s(3)): p-value = 2.92e-01\n",
        "\t- MultipleLines_Yes (s(14)): p-value = 3.15e-01\n",
        "\t- OnlineSecurity_Yes (s(18)): p-value = 5.28e-01\n",
        "\t- StreamingTV_No (s(20)): p-value = 1.07e-01\n",
        "\t- TechSupport_Yes (s(23)): p-value = 1.62e-01\n",
        "These features are not significant in predicting churning. Features like gender and seniorCitizen have been consistently unsignificant to all of our models. In order to increase our models performances, getting rid of these features during feature selection (or combining them) may lead to increased performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Linear Regression\n",
        "    - Strengths \n",
        "        - The coefficients are easy to interpret\n",
        "        - The importance of each feature can be inferred from the magnitude of each coefficient, where low values of coefficients indicate insignificance to the target value \n",
        "    - Weaknesses \n",
        "        - In our case, given that our target variable is binary, linear regression was not great at capturing the variance in the target value. \n",
        "        - The model assumes a linear relationship between features and the target variable, which is not necessarily the case for our dataset, especially that most of our features are categorical\n",
        "        - Cannot capture interactions between features unless explicitely included as interaction terms \n",
        "=> Linear Regression is simple, but is not appropriate for binary targets\n",
        "\n",
        "- Logistic Regression \n",
        "    - Strengths\n",
        "        - The coefficients are easy to interpret, although a bit more complicated than the linear regression's coefficients since we are dealing with log-odds now. The underlying principle is the same though. \n",
        "        - Just like with linear regression, the importance of each feature can be inferred from the magnitude of each coefficient, where low values of coefficients indicate insignificance to the target value \n",
        "        - In our case, given that our target variable is binary, logistic regression provided better insights and better performance \n",
        "    - Weaknesses\n",
        "        - The model assumes a linear relationship between features and the log-odds target variable, which is not necessarily the case for all features in our dataset as we saw \n",
        "        - Cannot capture interactions between features unless explicitely included as interaction terms \n",
        "=> Logistic Regression offers straightforward interpretability but may miss complex relationships\n",
        "\n",
        "- GAM\n",
        "    - Strengths\n",
        "        - Can capture non-linear relationships, which is more realistic since most relationships in the world are not linear \n",
        "        - Can automatically capture some interactions using the basis functions \n",
        "    - Weaknesses\n",
        "        - More complex and less interpretable compared to the other two models. However, using partial dependency plots and the p-values generated in the summary, we can still understand the significance of each feature in determining whether customers are churning \n",
        "=> Gam provides a good balance between flexibility and interpretability, capturing non-linear relationships while still allowing feature-level interpretation, by using dependency plots for example\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since our goal is to predict the factors that contribute the most to churn, while keeping in mind interpretability, we should go with logistic regression. It is true that the GAM model can capture interactions automatically, unlike the logistic regression model that requires them to be explicitely included. However, given that the models' performance are very similar, the increased complexity of the GAM model may not be justified. Using Linear Regression in our case is not appropriate since our target value is binary. As we saw earlier, there was a significance in the magnitude associated to the features between all models, with linear regression assigning the smallest coefficients.\n",
        "\n",
        "Note: this conclusion may change, leaning towards GAM, if better feature selection was applied."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMtVDE+z81yF7oDhaXsECyb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
